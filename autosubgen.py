# autosubgen.py
# -*- coding: utf-8 -*-

import os
import sys
import logging
import json
import time
import torch
import whisper
import pysubs2
import psutil
import threading
from tqdm import tqdm
from typing import List, Dict, Any
import config

# å¼•å…¥ä¸åŒå» å•†çš„åº«
from openai import OpenAI, AuthenticationError, RateLimitError, APIConnectionError
import google.generativeai as genai
from google.api_core import exceptions as google_exceptions
import anthropic

# è¨­å®š Logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# ==========================================
# 1. å®šç¾©æŠ½è±¡ä»‹é¢èˆ‡å…·é«”å¯¦ä½œ (Backend Adapters)
# ==========================================

class LLMBackend:
    """æ‰€æœ‰ LLM æä¾›è€…çš„åŸºé¡ (Interface)"""
    def check_health(self):
        raise NotImplementedError
    
    def process_batch(self, system_prompt: str, user_content: str) -> str:
        raise NotImplementedError

class OpenAIBackend(LLMBackend):
    def __init__(self):
        if not config.OPENAI_API_KEY:
            raise ValueError("ç¼ºå°‘ OPENAI_API_KEY")
        self.client = OpenAI(api_key=config.OPENAI_API_KEY)
        self.model = config.MODELS["openai"]

    def check_health(self):
        try:
            self.client.models.list() # ç°¡å–®é©—è­‰
        except Exception as e:
            raise ConnectionError(f"OpenAI é€£ç·šå¤±æ•—: {e}")

    def process_batch(self, system_prompt: str, user_content: str) -> str:
        response = self.client.chat.completions.create(
            model=self.model,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_content}
            ],
            temperature=0.3,
            response_format={"type": "json_object"}
        )
        return response.choices[0].message.content

class GeminiBackend(LLMBackend):
    def __init__(self):
        if not config.GEMINI_API_KEY:
            raise ValueError("ç¼ºå°‘ GEMINI_API_KEY")
        genai.configure(api_key=config.GEMINI_API_KEY)
        self.model_name = config.MODELS["gemini"]
        # Gemini çš„è¨­å®š
        self.generation_config = {
            "temperature": 0.3,
            "response_mime_type": "application/json" # å¼·åˆ¶ JSON
        }

    def check_health(self):
        try:
            model = genai.GenerativeModel(self.model_name)
            model.generate_content("Hi")
        except Exception as e:
            raise ConnectionError(f"Gemini é€£ç·šå¤±æ•—: {e}")

    def process_batch(self, system_prompt: str, user_content: str) -> str:
        model = genai.GenerativeModel(
            model_name=self.model_name,
            system_instruction=system_prompt,
            generation_config=self.generation_config
        )
        response = model.generate_content(user_content)
        return response.text

class ClaudeBackend(LLMBackend):
    def __init__(self):
        if not config.CLAUDE_API_KEY:
            raise ValueError("ç¼ºå°‘ CLAUDE_API_KEY")
        self.client = anthropic.Anthropic(api_key=config.CLAUDE_API_KEY)
        self.model = config.MODELS["claude"]

    def check_health(self):
        try:
            self.client.messages.create(
                model=self.model, max_tokens=1, messages=[{"role": "user", "content": "Hi"}]
            )
        except Exception as e:
            raise ConnectionError(f"Claude é€£ç·šå¤±æ•—: {e}")

    def process_batch(self, system_prompt: str, user_content: str) -> str:
        # Claude æ²’æœ‰åŸç”Ÿçš„ json_object æ¨¡å¼åƒæ•¸ï¼Œä½†åœ¨ Prompt ä¸­å¼·èª¿å³å¯ï¼Œæˆ–ä½¿ç”¨ prefill
        message = self.client.messages.create(
            model=self.model,
            max_tokens=4096,
            temperature=0.3,
            system=system_prompt + " Output must be valid JSON.",
            messages=[{"role": "user", "content": user_content}]
        )
        return message.content[0].text

# ==========================================
# 2. ä¸»ç¨‹å¼é‚è¼¯ (AutoSubGen)
# ==========================================

class AutoSubGen:
    def __init__(self, provider: str = "openai"):
        """
        åˆå§‹åŒ–ï¼šè¨­å®šè¨ˆç®—è¨­å‚™ä¸¦è¼‰å…¥æŒ‡å®šçš„ LLM Provider
        """
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        logger.info(f"æ­£åœ¨ä½¿ç”¨è¨ˆç®—è¨­å‚™: {self.device}")
        
        # --- Factory Pattern: æ ¹æ“šé¸æ“‡å¯¦ä¾‹åŒ–ä¸åŒçš„ Backend ---
        logger.info(f"æ­£åœ¨åˆå§‹åŒ– LLM Provider: {provider.upper()}...")
        try:
            if provider == "openai":
                self.llm = OpenAIBackend()
            elif provider == "gemini":
                self.llm = GeminiBackend()
            elif provider == "claude":
                self.llm = ClaudeBackend()
            else:
                raise ValueError("ä¸æ”¯æ´çš„ Provider")
            
            # çµ±ä¸€é€²è¡Œå¥åº·æª¢æŸ¥
            self.llm.check_health()
            logger.info(f"âœ… {provider.upper()} API é€£ç·šé©—è­‰æˆåŠŸï¼")
            
        except Exception as e:
            logger.critical(f"ğŸ›‘ API åˆå§‹åŒ–å¤±æ•—: {e}")
            logger.critical("è«‹æª¢æŸ¥ config.py ä¸­çš„ Key æ˜¯å¦æ­£ç¢ºã€‚")
            sys.exit(1)

        self.whisper_model = None
        self._stop_monitoring = False  # æ§åˆ¶ç›£æ§åŸ·è¡Œç·’çš„æ¨™èªŒ

    # (generate_output_paths, _load_whisper_model, transcribe_and_refine, translate_subtitles 
    #  é€™äº›æ–¹æ³•é‚è¼¯ä¸è®Šï¼Œé™¤äº†èª¿ç”¨ _process_text_with_gpt æ™‚ä¸éœ€è¦æ”¹å‹•)
    
    # çœç•¥é‡è¤‡ä»£ç¢¼ï¼Œåªåˆ—å‡ºèˆ‡ LLM äº’å‹•ç›¸é—œçš„ä¿®æ”¹...
    # --- æ–°å¢ï¼šç³»çµ±è³‡æºç›£æ§æ–¹æ³• ---
    def _monitor_resources(self):
        """å¾Œè‡ºåŸ·è¡Œç·’ï¼šæ¯éš”å¹¾ç§’å°å‡ºç³»çµ±è¨˜æ†¶é«”ä½”ç”¨"""
        while not self._stop_monitoring:
            # ç²å–è¨˜æ†¶é«”è³‡è¨Š
            mem = psutil.virtual_memory()
            total_gb = mem.total / (1024 ** 3)
            used_gb = mem.used / (1024 ** 3)
            percent = mem.percent
            
            # ä½¿ç”¨ \033 é¡è‰²ä»£ç¢¼è®“å®ƒé¡¯çœ¼ä¸€é» (é’è‰²)
            # æ ¼å¼ï¼š[System] RAM: 8.5GB / 32.0GB (26.5%)
            print(f"\033[96m[System Monitor] RAM Usage: {used_gb:.2f}GB / {total_gb:.2f}GB ({percent}%)\033[0m")
            
            # æ¯ 3 ç§’æ›´æ–°ä¸€æ¬¡
            time.sleep(3)
    
    # å¿…é ˆå®Œæ•´ä¿ç•™ generate_output_paths
    def generate_output_paths(self, video_path: str) -> dict:
        base_name = os.path.splitext(os.path.basename(video_path))[0]
        save_dir_config = config.OUTPUT_SETTINGS.get("save_dir", "")
        if save_dir_config:
            output_dir = os.path.abspath(save_dir_config)
        else:
            output_dir = os.path.dirname(os.path.abspath(video_path))
        if not os.path.exists(output_dir):
            os.makedirs(output_dir, exist_ok=True)
        return {
            "en": os.path.join(output_dir, f"{base_name}{config.OUTPUT_SETTINGS['suffix_en']}"),
            "zh": os.path.join(output_dir, f"{base_name}{config.OUTPUT_SETTINGS['suffix_zh']}"),
            "merge": os.path.join(output_dir, f"{base_name}{config.OUTPUT_SETTINGS['suffix_merge']}")
        }
    
    # å¿…é ˆå®Œæ•´ä¿ç•™ _load_whisper_model
    def _load_whisper_model(self):
        if self.whisper_model is None:
            logger.info(f"æ­£åœ¨åŠ è¼‰ Whisper æ¨¡å‹ ({config.WHISPER_MODEL_SIZE})...")
            if os.path.exists(config.WHISPER_MODEL_SIZE):
                logger.info(f"æª¢æ¸¬åˆ°æœ¬åœ°æ¨¡å‹æ–‡ä»¶: {config.WHISPER_MODEL_SIZE}")
            self.whisper_model = whisper.load_model(config.WHISPER_MODEL_SIZE, device=self.device)

    # è½‰éŒ„æ–¹æ³• (ä¿ç•™ verbose=True)
    def transcribe_and_refine(self, video_path: str, output_path: str):
        if not os.path.exists(video_path): raise FileNotFoundError(f"æ‰¾ä¸åˆ°: {video_path}")
        self._load_whisper_model()
        logger.info(f"é–‹å§‹è½‰éŒ„: {video_path}")

        # --- 1. å•Ÿå‹•ç›£æ§åŸ·è¡Œç·’ ---
        self._stop_monitoring = False
        monitor_thread = threading.Thread(target=self._monitor_resources)
        monitor_thread.daemon = True 
        monitor_thread.start()
        
        # --- 2. ä½¿ç”¨ try...finally ç¢ºä¿ç›£æ§æœƒåœæ­¢ ---
        try:
            # åŸ·è¡Œè€—æ™‚çš„ Whisper è½‰éŒ„
            result = self.whisper_model.transcribe(
                video_path, 
                language="en", 
                fp16=(self.device=="cuda"), 
                verbose=True
            )
        finally:
            # --- 3. ç„¡è«–æˆåŠŸæˆ–å¤±æ•—ï¼Œé€™è£éƒ½æœƒåŸ·è¡Œï¼Œåœæ­¢ç›£æ§ ---
            self._stop_monitoring = True
            monitor_thread.join() # ç­‰å¾…åŸ·è¡Œç·’ä¹¾æ·¨åœ°çµæŸ
            print("\n") # å°å€‹æ›è¡Œï¼Œè®“ç‰ˆé¢å¥½çœ‹é»

        #result = self.whisper_model.transcribe(video_path, language="en", fp16=(self.device=="cuda"), verbose=True)
        
        segments = result['segments']
        logger.info(f"\nâœ… è½‰éŒ„å®Œæˆï¼Œå…± {len(segments)} è¡Œã€‚")
        
        subs = pysubs2.SSAFile()
        raw_texts = []
        for seg in segments:
            evt = pysubs2.SSAEvent(start=int(seg['start']*1000), end=int(seg['end']*1000), text=seg['text'].strip())
            subs.events.append(evt)
            raw_texts.append(evt.text)
            
        logger.info("æ­£åœ¨é€²è¡Œæ½¤é£¾...")
        refined = self._process_text_unified(raw_texts, "refine") # æ”¹åèª¿ç”¨çµ±ä¸€æ–¹æ³•
        
        for i, txt in enumerate(refined[:len(subs.events)]): subs.events[i].text = txt
        subs.save(output_path)
        logger.info(f"å·²ä¿å­˜: {output_path}")

    # ç¿»è­¯æ–¹æ³•
    def translate_subtitles(self, input_path: str, output_path: str):
        if not os.path.exists(input_path): raise FileNotFoundError(f"æ‰¾ä¸åˆ°: {input_path}")
        subs = pysubs2.load(input_path)
        logger.info("æ­£åœ¨é€²è¡Œç¿»è­¯...")
        translated = self._process_text_unified([e.text for e in subs.events], "translate")
        
        for i, txt in enumerate(translated[:len(subs.events)]): subs.events[i].text = txt
        subs.save(output_path)
        logger.info(f"å·²ä¿å­˜: {output_path}")

    # åˆä½µæ–¹æ³• (ä¿æŒä¸è®Š)
    # åœ¨ autosubgen.py ä¸­æ›¿æ›/æ–°å¢ä»¥ä¸‹æ–¹æ³•

    def merge_subtitles(self, zh_path: str, en_path: str, output_path: str):
        
        logger.info("é–‹å§‹åˆä½µé›™èªå­—å¹• (æ¨£å¼å¾©åˆ»æ¨¡å¼)...")
        
        try:
            subs_zh = pysubs2.load(zh_path)
            subs_en = pysubs2.load(en_path)
        except Exception as e:
            logger.error(f"è®€å–å­—å¹•æ–‡ä»¶å¤±æ•—: {e}")
            return

        # 1. å»ºç«‹æ–°çš„å­—å¹•æª”ï¼Œä¸¦è¨­å®š Header åƒæ•¸ (PlayRes)
        merged_subs = pysubs2.SSAFile()
        merged_subs.info.update(config.ASS_PARAMS) # å¯«å…¥ 384x288 è§£æåº¦è¨­å®š

        # 2. è¼‰å…¥ä¸¦è¨»å†Šæ¨£å¼
        style_main_cfg = config.STYLE_CONFIG["main"]
        style_sec_cfg = config.STYLE_CONFIG["second"]

        merged_subs.styles[style_main_cfg["Name"]] = self._create_pysubs2_style(style_main_cfg)
        merged_subs.styles[style_sec_cfg["Name"]] = self._create_pysubs2_style(style_sec_cfg)

        # 3. åˆä½µé‚è¼¯
        # ç”±æ–¼ Whisper ç”Ÿæˆçš„æ™‚é–“è»¸éå¸¸ç²¾æº–ï¼Œä¸­è‹±è¡Œæ•¸é€šå¸¸ä¸€è‡´ã€‚
        # ç‚ºäº†ä¿éšªï¼Œæˆ‘å€‘ä½¿ç”¨æ™‚é–“æˆ³è¨˜ä¾†å°‹æ‰¾å°æ‡‰çš„è‹±æ–‡å­—å¹•ï¼Œè€Œä¸æ˜¯å‡è¨­è¡Œè™Ÿå°æ‡‰ã€‚
        
        # å»ºç«‹è‹±æ–‡äº‹ä»¶çš„ç´¢å¼•åŠ é€ŸæŸ¥æ‰¾
        # ç°¡å–®ç­–ç•¥ï¼šå‡è¨­è¡Œæ•¸å¤§è‡´å°æ‡‰ï¼Œè‹¥ä¸å°æ‡‰å‰‡å°‹æ‰¾æ™‚é–“é‡ç–Šæœ€å¤§çš„
        
        logger.info(f"æ­£åœ¨åˆä½µ {len(subs_zh)} è¡Œå­—å¹•...")
        
        # ç‚ºäº†è™•ç†å…©è€…è¡Œæ•¸ä¸ä¸€è‡´çš„æƒ…æ³ï¼Œæˆ‘å€‘éæ­·ä¸­æ–‡ï¼Œå»è‹±æ–‡è£æ‰¾å°æ‡‰
        for z_event in subs_zh.events:
            z_start = z_event.start
            z_end = z_event.end
            z_text = z_event.text.strip()
            
            # å°‹æ‰¾æ™‚é–“é‡ç–Šæœ€å¤šçš„è‹±æ–‡å¥å­
            best_match_en = ""
            max_overlap = 0
            
            for e_event in subs_en.events:
                # è¨ˆç®—é‡ç–Šæ™‚é–“
                overlap_start = max(z_start, e_event.start)
                overlap_end = min(z_end, e_event.end)
                overlap = overlap_end - overlap_start
                
                if overlap > max_overlap:
                    max_overlap = overlap
                    best_match_en = e_event.text.strip()
                
                # å„ªåŒ–ï¼šå¦‚æœä½ å·²ç¶“éäº†é€™æ®µæ™‚é–“ï¼Œå°±ä¸ç”¨å†å¾€ä¸‹æ‰¾äº† (å‡è¨­æ˜¯æœ‰åºçš„)
                if e_event.start > z_end:
                    break
            
            # 4. æ§‹å»ºé›™èªå…§å®¹
            # æ ¼å¼ï¼šä¸­æ–‡\N{\rEng}English
            # \N æ˜¯æ›è¡Œï¼Œ{\rEng} æ˜¯å¼·åˆ¶é‡ç½®è©²è¡Œå‰©é¤˜éƒ¨åˆ†çš„æ¨£å¼ç‚º "Eng"
            if best_match_en:
                final_text = f"{z_text}\\N{{\\r{style_sec_cfg['Name']}}}{best_match_en}"
            else:
                final_text = z_text # æ²’æ‰¾åˆ°è‹±æ–‡å°±åªæ”¾ä¸­æ–‡

            # å»ºç«‹æ–°äº‹ä»¶ï¼Œä½¿ç”¨ä¸»æ¨£å¼ (Default)
            new_event = pysubs2.SSAEvent(
                start=z_start,
                end=z_end,
                text=final_text,
                style=style_main_cfg["Name"]
            )
            merged_subs.events.append(new_event)

        merged_subs.save(output_path)
        logger.info(f"âœ… é›™èªå­—å¹•åˆä½µå®Œæˆï¼Œå·²ä¿å­˜ç‚º: {output_path}")

    def _create_pysubs2_style(self, cfg: dict) -> pysubs2.SSAStyle:
        """
        å°‡ config å­—å…¸è½‰æ›ç‚º pysubs2.SSAStyle ç‰©ä»¶
        """
        style = pysubs2.SSAStyle()
        style.fontname = cfg.get("Fontname", "Arial")
        style.fontsize = cfg.get("Fontsize", 20)
        style.primarycolor = pysubs2.Color(*self._parse_ass_color(cfg.get("PrimaryColour")))
        style.secondarycolor = pysubs2.Color(*self._parse_ass_color(cfg.get("SecondaryColour")))
        style.outlinecolor = pysubs2.Color(*self._parse_ass_color(cfg.get("OutlineColour")))
        style.backcolor = pysubs2.Color(*self._parse_ass_color(cfg.get("BackColour")))
        style.bold = cfg.get("Bold", 0)
        style.italic = cfg.get("Italic", 0)
        style.borderstyle = cfg.get("BorderStyle", 1)
        style.outline = cfg.get("Outline", 2)
        style.shadow = cfg.get("Shadow", 0)
        style.alignment = cfg.get("Alignment", 2)
        style.marginl = cfg.get("MarginL", 10)
        style.marginr = cfg.get("MarginR", 10)
        style.marginv = cfg.get("MarginV", 10)
        style.encoding = cfg.get("Encoding", 1)
        return style

    def _parse_ass_color(self, ass_hex: str):
        hex_str = ass_hex.replace("&H", "")
        if len(hex_str) != 8: return 255, 255, 255, 0
        return int(hex_str[6:8], 16), int(hex_str[4:6], 16), int(hex_str[2:4], 16), int(hex_str[0:2], 16)

    # ==========================================
    # 3. çµ±ä¸€çš„è™•ç†æ ¸å¿ƒ (Unified Processor)
    # ==========================================
    
    def _process_text_unified(self, texts: List[str], mode: str) -> List[str]:
        """
        çµ±ä¸€è™•ç†é‚è¼¯ï¼šè² è²¬ Batch åˆ‡åˆ†ã€é‡è©¦å¾ªç’°ã€èª¿ç”¨å¾Œç«¯
        """
        processed_texts = []
        batch_size = 20
        max_retries = 3
        
        if mode == "refine":
            sys_prompt = "Correct grammar/punctuation. Return a JSON list of strings."
        else:
            sys_prompt = "Translate to Traditional Chinese (Taiwan). Return a JSON list of strings."

        for i in tqdm(range(0, len(texts), batch_size), desc=f"AI Processing ({mode})"):
            batch = texts[i : i + batch_size]
            user_content = json.dumps(batch, ensure_ascii=False)
            
            # é‡è©¦æ©Ÿåˆ¶
            for attempt in range(max_retries):
                try:
                    # èª¿ç”¨å¤šæ…‹çš„ llm.process_batch
                    response_text = self.llm.process_batch(sys_prompt, f"Process: {user_content}")
                    
                    # å˜—è©¦è§£æ JSON
                    try:
                        # æ¸…ç†å¯èƒ½å­˜åœ¨çš„ Markdown code block (ä¾‹å¦‚ ```json ... ```)
                        clean_text = response_text.replace("```json", "").replace("```", "").strip()
                        data = json.loads(clean_text)
                        
                        if isinstance(data, dict):
                            batch_result = list(data.values())[0] if data.values() else batch
                        elif isinstance(data, list):
                            batch_result = data
                        else:
                            batch_result = batch
                    except json.JSONDecodeError:
                        logger.warning(f"Batch {i} JSON è§£æå¤±æ•—ï¼Œå˜—è©¦ä¿®å¾©æˆ–æ”¾æ£„...")
                        batch_result = batch

                    if len(batch_result) != len(batch):
                        batch_result = batch
                        
                    processed_texts.extend(batch_result)
                    break # æˆåŠŸå‰‡è·³å‡ºé‡è©¦

                except Exception as e:
                    # çµ±ä¸€æ•æ‰å„å®¶ API çš„éŒ¯èª¤
                    logger.warning(f"API Error (Attempt {attempt+1}): {e}")
                    if attempt < max_retries - 1:
                        time.sleep(2 ** attempt)
                    else:
                        logger.error(f"Batch {i} æœ€çµ‚å¤±æ•—ï¼Œä½¿ç”¨åŸæ–‡ã€‚")
                        processed_texts.extend(batch)
        
        return processed_texts