# autosubgen.py
# -*- coding: utf-8 -*-

import os
import sys
import logging
import json
import time
import torch
import whisper
import pysubs2
import psutil
import threading
from tqdm import tqdm
from typing import List, Dict, Any
import config

# ÂºïÂÖ•‰∏çÂêåÂª†ÂïÜÁöÑÂ∫´
from openai import OpenAI, AuthenticationError, RateLimitError, APIConnectionError
import google.generativeai as genai
from google.api_core import exceptions as google_exceptions
import anthropic

# Ë®≠ÂÆö Logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# ==========================================
# 1. ÂÆöÁæ©ÊäΩË±°‰ªãÈù¢ËàáÂÖ∑È´îÂØ¶‰Ωú (Backend Adapters)
# ==========================================

class LLMBackend:
    """ÊâÄÊúâ LLM Êèê‰æõËÄÖÁöÑÂü∫È°û (Interface)"""
    def check_health(self):
        raise NotImplementedError
    
    def process_batch(self, system_prompt: str, user_content: str) -> str:
        raise NotImplementedError

class OpenAIBackend(LLMBackend):
    def __init__(self):
        if not config.OPENAI_API_KEY:
            raise ValueError("Áº∫Â∞ë OPENAI_API_KEY")
        self.client = OpenAI(api_key=config.OPENAI_API_KEY)
        self.model = config.MODELS["openai"]

    def check_health(self):
        try:
            self.client.models.list() 
        except Exception as e:
            raise ConnectionError(f"OpenAI ÈÄ£Á∑öÂ§±Êïó: {e}")

    def process_batch(self, system_prompt: str, user_content: str) -> str:
        response = self.client.chat.completions.create(
            model=self.model,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_content}
            ],
            temperature=0.3,
            response_format={"type": "json_object"}
        )
        return response.choices[0].message.content

class GeminiBackend(LLMBackend):
    def __init__(self):
        if not config.GEMINI_API_KEY:
            raise ValueError("Áº∫Â∞ë GEMINI_API_KEY")
        genai.configure(api_key=config.GEMINI_API_KEY)
        self.model_name = config.MODELS["gemini"]
        self.generation_config = {
            "temperature": 0.3,
            "response_mime_type": "application/json"
        }

    def check_health(self):
        try:
            model = genai.GenerativeModel(self.model_name)
            model.generate_content("Hi")
        except Exception as e:
            raise ConnectionError(f"Gemini ÈÄ£Á∑öÂ§±Êïó: {e}")

    def process_batch(self, system_prompt: str, user_content: str) -> str:
        model = genai.GenerativeModel(
            model_name=self.model_name,
            system_instruction=system_prompt,
            generation_config=self.generation_config
        )
        response = model.generate_content(user_content)
        return response.text

class ClaudeBackend(LLMBackend):
    def __init__(self):
        if not config.CLAUDE_API_KEY:
            raise ValueError("Áº∫Â∞ë CLAUDE_API_KEY")
        self.client = anthropic.Anthropic(api_key=config.CLAUDE_API_KEY)
        self.model = config.MODELS["claude"]

    def check_health(self):
        try:
            self.client.messages.create(
                model=self.model, max_tokens=1, messages=[{"role": "user", "content": "Hi"}]
            )
        except Exception as e:
            raise ConnectionError(f"Claude ÈÄ£Á∑öÂ§±Êïó: {e}")

    def process_batch(self, system_prompt: str, user_content: str) -> str:
        message = self.client.messages.create(
            model=self.model,
            max_tokens=4096,
            temperature=0.3,
            system=system_prompt + " Output must be valid JSON.",
            messages=[{"role": "user", "content": user_content}]
        )
        return message.content[0].text

# ==========================================
# 2. ‰∏ªÁ®ãÂºèÈÇèËºØ (AutoSubGen)
# ==========================================

class AutoSubGen:
    def __init__(self, provider: str = "openai"):
        """ÂàùÂßãÂåñÔºöË®≠ÂÆöË®àÁÆóË®≠ÂÇô‰∏¶ËºâÂÖ•ÊåáÂÆöÁöÑ LLM Provider"""
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        logger.info(f"Ê≠£Âú®‰ΩøÁî®Ë®àÁÆóË®≠ÂÇô: {self.device}")
        
        logger.info(f"Ê≠£Âú®ÂàùÂßãÂåñ LLM Provider: {provider.upper()}...")
        try:
            if provider == "openai":
                self.llm = OpenAIBackend()
            elif provider == "gemini":
                self.llm = GeminiBackend()
            elif provider == "claude":
                self.llm = ClaudeBackend()
            else:
                raise ValueError("‰∏çÊîØÊè¥ÁöÑ Provider")
            
            self.llm.check_health()
            logger.info(f"‚úÖ {provider.upper()} API ÈÄ£Á∑öÈ©óË≠âÊàêÂäüÔºÅ")
            
        except Exception as e:
            logger.critical(f"üõë API ÂàùÂßãÂåñÂ§±Êïó: {e}")
            logger.critical("Ë´ãÊ™¢Êü• config.py ‰∏≠ÁöÑ Key ÊòØÂê¶Ê≠£Á¢∫„ÄÇ")
            sys.exit(1)

        self.whisper_model = None
        self._stop_monitoring = False

    # --- Á≥ªÁµ±Ë≥áÊ∫êÁõ£Êéß ---
    def _monitor_resources(self):
        while not self._stop_monitoring:
            mem = psutil.virtual_memory()
            total_gb = mem.total / (1024 ** 3)
            used_gb = mem.used / (1024 ** 3)
            percent = mem.percent
            print(f"\033[96m[System Monitor] RAM Usage: {used_gb:.2f}GB / {total_gb:.2f}GB ({percent}%)\033[0m")
            time.sleep(3)
    
    def generate_output_paths(self, video_path: str) -> dict:
        base_name = os.path.splitext(os.path.basename(video_path))[0]
        save_dir_config = config.OUTPUT_SETTINGS.get("save_dir", "")
        if save_dir_config:
            output_dir = os.path.abspath(save_dir_config)
        else:
            output_dir = os.path.dirname(os.path.abspath(video_path))
        if not os.path.exists(output_dir):
            os.makedirs(output_dir, exist_ok=True)
        return {
            "en": os.path.join(output_dir, f"{base_name}{config.OUTPUT_SETTINGS['suffix_en']}"),
            "zh": os.path.join(output_dir, f"{base_name}{config.OUTPUT_SETTINGS['suffix_zh']}"),
            "merge": os.path.join(output_dir, f"{base_name}{config.OUTPUT_SETTINGS['suffix_merge']}")
        }
    
    def _load_whisper_model(self):
        if self.whisper_model is None:
            logger.info(f"Ê≠£Âú®Âä†Ëºâ Whisper Ê®°Âûã ({config.WHISPER_MODEL_SIZE})...")
            if os.path.exists(config.WHISPER_MODEL_SIZE):
                logger.info(f"Ê™¢Ê∏¨Âà∞Êú¨Âú∞Ê®°ÂûãÊñá‰ª∂: {config.WHISPER_MODEL_SIZE}")
            self.whisper_model = whisper.load_model(config.WHISPER_MODEL_SIZE, device=self.device)

    # --- ÂäüËÉΩ‰∏ÄÔºöËΩâÈåÑËàáÊΩ§È£æ (‰øÆÊ≠£Áõ£ÊéßÈÇèËºØ) ---
    def transcribe_and_refine(self, video_path: str, output_path: str):
        if not os.path.exists(video_path): raise FileNotFoundError(f"Êâæ‰∏çÂà∞: {video_path}")
        
        self._load_whisper_model()
        logger.info(f"ÈñãÂßãËΩâÈåÑ: {video_path}")

        # 1. ÂïüÂãïÁõ£Êéß
        self._stop_monitoring = False
        monitor_thread = threading.Thread(target=self._monitor_resources)
        monitor_thread.daemon = True 
        monitor_thread.start()
        
        result = None

        # 2. Âü∑Ë°åËΩâÈåÑ (Âê´ÈåØË™§Èò≤Ë≠∑)
        try:
            result = self.whisper_model.transcribe(
                video_path, 
                language="en", 
                fp16=(self.device=="cuda"), 
                verbose=True
            )
        finally:
            # 3. ÂÅúÊ≠¢Áõ£Êéß
            self._stop_monitoring = True
            monitor_thread.join()
            print("\n") 

        # 4. ÂæåÁ∫åËôïÁêÜ
        if result:
            segments = result['segments']
            logger.info(f"‚úÖ ËΩâÈåÑÂÆåÊàêÔºåÂÖ± {len(segments)} Ë°å„ÄÇ")
            
            subs = pysubs2.SSAFile()
            raw_texts = []
            for seg in segments:
                evt = pysubs2.SSAEvent(start=int(seg['start']*1000), end=int(seg['end']*1000), text=seg['text'].strip())
                subs.events.append(evt)
                raw_texts.append(evt.text)
            
            logger.info("Ê≠£Âú®ÈÄ≤Ë°åÊΩ§È£æ...")
            # ÊΩ§È£æ‰∏çÈúÄË¶ÅÂÖ®Â±ÄÂàÜÊûêÔºåÁõ¥Êé•Ë™øÁî®
            refined = self._process_text_unified(raw_texts, "refine") 
            
            min_len = min(len(refined), len(subs.events))
            for i in range(min_len):
                subs.events[i].text = refined[i]
                
            subs.save(output_path)
            logger.info(f"Â∑≤‰øùÂ≠ò: {output_path}")

    # --- ÂäüËÉΩ‰∫åÔºöÁøªË≠Ø (ÂÖ©ÈöéÊÆµÊµÅÁ®ãÔºöÂàÜÊûê -> ÁøªË≠Ø) ---
    def translate_subtitles(self, input_path: str, output_path: str):
        if not os.path.exists(input_path): raise FileNotFoundError(f"Êâæ‰∏çÂà∞: {input_path}")
        
        subs = pysubs2.load(input_path)
        original_texts = [event.text for event in subs.events]
        
        # --- Step 1: Âü∑Ë°åÂÖ®Â±ÄÂàÜÊûê (Pass 1) ---
        global_context_str = self._analyze_global_context(original_texts)
        
        # --- Step 2: Âü∑Ë°åÁøªË≠Ø (Pass 2) ---
        logger.info("Ê≠£Âú®ÈÄ≤Ë°åÁ¨¨‰∫åÈöéÊÆµÔºöÈÄêÊÆµÁøªË≠Ø (Pass 2)...")
        translated_texts = self._process_text_unified(
            original_texts, 
            mode="translate",
            global_context=global_context_str # ÂÇ≥ÂÖ•ËÅñÁ∂ì
        )
        
        min_len = min(len(translated_texts), len(subs.events))
        for i in range(min_len):
            subs.events[i].text = translated_texts[i]
            
        subs.save(output_path)
        logger.info(f"Â∑≤‰øùÂ≠ò: {output_path}")

    # --- Pass 1: ÂÖ®Â±ÄÂàÜÊûêÊ†∏ÂøÉ ---
    def _analyze_global_context(self, all_texts: List[str]) -> str:
        """Èñ±ËÆÄÂÆåÊï¥ÂäáÊú¨ÔºåÁîüÊàêÂäáÊÉÖÂ§ßÁ∂±ËàáËßíËâ≤Èóú‰øÇË°® (ÁøªË≠ØËÅñÁ∂ì)"""
        logger.info("Ê≠£Âú®ÈÄ≤Ë°åÁ¨¨‰∏ÄÈöéÊÆµÔºöÂÖ®Â±ÄÂäáÊÉÖËàáËßíËâ≤ÂàÜÊûê (ÁîüÊàêÁøªË≠ØËÅñÁ∂ì)...")
        
        # ÈôêÂà∂Èï∑Â∫¶Èò≤ÂëÜ
        full_script = "\n".join(all_texts)[:100000]
        
        analysis_prompt = (
            "You are a lead localization expert. Read the provided movie script/subtitles below.\n"
            "Create a concise 'Translation Bible' to guide the translators.\n"
            "Output strictly a JSON object (no markdown) with the following keys:\n"
            "1. 'summary': A 3-sentence plot summary.\n"
            "2. 'tone': The overall tone (e.g., Serious, Comedic, Formal, Slang-heavy).\n"
            "3. 'characters': A list of main characters with their GENDER (Male/Female) and RELATIONSHIPS (e.g., 'A is B's boss', 'C and D are lovers'). This is crucial for Chinese pronouns (‰ªñ/Â•π) and honorifics (‰Ω†/ÊÇ®).\n"
            "4. 'key_terms': Key proper nouns or jargon that need consistent translation.\n"
        )

        try:
            response = self.llm.process_batch(analysis_prompt, f"Script Content:\n{full_script}")
            cleaned_response = response.replace("```json", "").replace("```", "").strip()
            logger.info("‚úÖ ÁøªË≠ØËÅñÁ∂ìÁîüÊàêÂÆåÊàê„ÄÇ")
            return cleaned_response
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è ÂÖ®Â±ÄÂàÜÊûêÂ§±Êïó: {e}„ÄÇÂ∞á‰ΩøÁî®ÈÄöÁî®Ë¶èÂâáÈÄ≤Ë°åÁøªË≠Ø„ÄÇ")
            return ""

    # ‰øÆÊîπ autosubgen.py ‰∏≠ÁöÑÈÄôÂÄãÊñπÊ≥ï
    def _get_enhanced_system_prompt(self, mode: str, global_context: str = "") -> str:
        if mode == "refine":
            return (
                "You are a professional subtitle editor. "
                "Correct grammar, casing, and punctuation errors. "
                "Keep the subtitles concise. Do NOT change the meaning. "
                "Output strictly a JSON list of strings."
            )
        
        # Translate Ê®°Âºè (Âä†ÂÖ•Á¨¨ 7 ÈªûË¶èÂâáÔºöÂèçÁõ¥Ë≠ØËàáÈÇèËºØÊ™¢Êü•)
        base_prompt = (
            "‰Ω†ÊòØ Netflix Á≠âÁ¥öÁöÑË≥áÊ∑±Â≠óÂπïÁøªË≠ØÂì°ÔºåÂ∞àÁ≤æÊñºÂ∞áËã±ÊñáÁøªË≠ØÊàê„ÄåÊ≠£È´î‰∏≠ÊñáÔºàÂè∞ÁÅ£Ôºâ„Äç„ÄÇ\n"
            "‰Ω†ÁöÑ‰ªªÂãôÊòØÊ†πÊìö„ÄåÂäáÊÉÖËÉåÊôØ„ÄçËàá„Äå‰∏ä‰∏ãÊñá„ÄçÔºåÂ∞áËº∏ÂÖ•ÁöÑÂ≠óÂπïÁøªË≠ØÊàêÊµÅÊö¢„ÄÅËá™ÁÑ∂ÁöÑÂè∞ÁÅ£Âè£Ë™û„ÄÇ\n\n"
            "### Ê†∏ÂøÉÁøªË≠ØË¶èÂâá (ÂøÖÈ†àÈÅµÂÆà) ###\n"
            "1. **Âú®Âú∞ÂåñÁî®Ë™û**ÔºöÁµïÂ∞çÈÅøÂÖç‰∏≠ÂúãÂ§ßÈô∏Áî®Ë™ûÔºåÂøÖÈ†à‰ΩøÁî®Âè∞ÁÅ£ÁøíÊÖ£Áî®Ë™û„ÄÇ\n"
            "   - (‰æãÔºöË¶ñÈ†ª->ÂΩ±Áâá, Ë≥™Èáè->ÂìÅË≥™, È†ÖÁõÆ->Â∞àÊ°à, Ëªü‰ª∂->ËªüÈ´î, ‰ø°ÊÅØ->Ë≥áË®ä, ÈªòË™ç->È†êË®≠, Á∂≤Áµ°->Á∂≤Ë∑Ø)\n"
            "2. **Ë™ûÊ∞£ËàáÊï¨Ë™û**ÔºöÈÄôÁî±ËßíËâ≤Èóú‰øÇÊ±∫ÂÆö„ÄÇÂ∞ç‰∏äÁ¥öÊàñÈôåÁîü‰∫∫‰ΩøÁî®„ÄåÊÇ®„ÄçÔºåÂ∞çÂπ≥Ëº©Êàñ‰∏ãÂ±¨‰ΩøÁî®„Äå‰Ω†„Äç„ÄÇ\n"
            "3. **Á∞°ÊΩîÁ≤æÊ∫ñ**ÔºöÂ≠óÂπï‰∏çÂÉÖË¶ÅÊ∫ñÁ¢∫ÔºåÈÇÑË¶ÅÁ∞°Áü≠ÊúâÂäõÔºåÈÅ©ÂêàÈñ±ËÆÄ„ÄÇ\n"
            "4. **Â∞àÊúâÂêçË©û**ÔºöËã•ËÉåÊôØË®≠ÂÆö‰∏≠ÊúâÊåáÂÆöË≠ØÂêçÔºåË´ãÂö¥Ê†ºÈÅµÂÆà„ÄÇ\n"
            "5. **Ê†ºÂºèË¶ÅÊ±Ç**ÔºöÁµïÂ∞ç‰∏çË¶ÅËº∏Âá∫‰ªª‰ΩïËß£ÈáãÊàñMarkdownÊ®ôË®òÔºå**Âè™Ëº∏Âá∫Á¥î JSON Â≠ó‰∏≤ÂàóË°®**„ÄÇ\n"
            "6. **Ê®ôÈªûËàáÊéíÁâà**Ôºö‰∏≠ÊñáÂ≠óÂπïÂÖßÂÆπ**‰∏çÂèØÂåÖÂê´‰ªª‰ΩïÊ®ôÈªûÁ¨¶Ëôü**ÔºàÂ¶ÇÔºöÔºå„ÄÇÔºüÔºÅÔºâ„ÄÇËã•Âè•Â≠ê‰∏≠ÈñìÈúÄË¶ÅÂÅúÈ†ìÊàñÊñ∑Âè•ÔºåË´ãÂº∑Âà∂‰ΩøÁî®„ÄåÁ©∫Ê†º„Äç‰ª£ÊõøÔºõÂè•Â∞æ‰πü‰∏çË¶ÅÂä†Á¨¶Ëôü„ÄÇ\n"
            "7. **ÊãíÁµïÁõ¥Ë≠Ø (ÈóúÈçµ)**ÔºöÁøªË≠ØÂøÖÈ†àÂü∫ÊñºÊï¥Âè•ÈÇèËºØËàáË™ûÂ¢É„ÄÇÈÅáÂà∞Ëã±ÊñáÊÖ£Áî®Ë™û (Idioms)„ÄÅÂÄíË£ùÂè•ÊàñÂº∑Ë™øÂè•ÔºàÂ¶Ç 'for the life of me', 'over my dead body'ÔºâÔºå**ÂøÖÈ†àÊÑèË≠ØÂÖ∂„ÄåË®ÄÂ§ñ‰πãÊÑè„Äç**ÔºåÂö¥Á¶ÅÈÄêÂ≠óÁøªË≠ØÈÄ†ÊàêÈÇèËºØÈåØË™§„ÄÇ\n"
        )

        if global_context:
            base_prompt += (
                "\n### ÂäáÊÉÖËÉåÊôØËàáËßíËâ≤Èóú‰øÇ (Translation Bible) ###\n"
                "Ë´ãÂèÉËÄÉ‰ª•‰∏ãË®≠ÂÆö‰æÜÊ±∫ÂÆöÂ∞çË©±ÁöÑË™ûÊ∞£ÔºàÊï¨Ë™û/Á≤ó‰øó/Ê≠£ÂºèÔºâÔºö\n"
                "------------------------------------------------\n"
                f"{global_context}\n"
                "------------------------------------------------\n"
            )
        
        base_prompt += (
            "\n### ÂãïÊÖãËº∏ÂÖ•Ë™™Êòé ###\n"
            "‰Ω†Â∞áÊî∂Âà∞‰∏ÄÂÄã JSON Áâ©‰ª∂ÔºåÂåÖÂê´Ôºö\n"
            "- 'previous_context': ‰∏ä‰∏ÄÊÆµÂ∞çË©±ÂÖßÂÆπÔºàÂÉÖ‰æõÂèÉËÄÉÔºåÁî®ÊñºÈÄ£Ë≤´Ë™ûÊ∞£Ôºâ„ÄÇ\n"
            "- 'lines_to_process': ÈúÄË¶ÅÁøªË≠ØÁöÑËã±ÊñáÂè•Â≠êÂàóË°®„ÄÇ\n\n"
            "Ë´ãÂèÉËÄÉ 'previous_context' ÁöÑË™ûÂ¢ÉÔºåÂÉÖÁøªË≠Ø 'lines_to_process' ÈÉ®ÂàÜ„ÄÇ"
        )
        return base_prompt
    
    # --- Pass 2: Áµ±‰∏ÄËôïÁêÜÊ†∏ÂøÉ (ÊªëÂãïÁ™óÂè£ + Ê≥®ÂÖ• Prompt) ---
    def _process_text_unified(self, texts: List[str], mode: str, global_context: str = "") -> List[str]:
        processed_texts = []
        batch_size = 20
        max_retries = 3
        
        sys_prompt = self._get_enhanced_system_prompt(mode, global_context)
        previous_context = [] # ÊªëÂãïÁ™óÂè£

        for i in tqdm(range(0, len(texts), batch_size), desc=f"AI Processing ({mode})"):
            batch = texts[i : i + batch_size]
            
            # Âª∫ÊßãËº∏ÂÖ•Ë≥áÊñôÔºöÂåÖÂê´‰∏ä‰∏ãÊñá + Êú¨Ê¨°Ë¶ÅÁøªË≠ØÁöÑÂè•Â≠ê
            context_str = "\n".join(previous_context) if previous_context else "ÁÑ° (Â∞çË©±ÈñãÂßã)"
            user_content_obj = {
                "previous_context": context_str,
                "lines_to_process": batch
            }
            
            # Refine Ê®°Âºè‰ΩøÁî®Á∞°ÂñÆ JSONÔºåTranslate Ê®°Âºè‰ΩøÁî®Â∏∂‰∏ä‰∏ãÊñáÁöÑ JSON
            if mode == "refine":
                user_content_str = json.dumps(batch, ensure_ascii=False)
            else:
                user_content_str = json.dumps(user_content_obj, ensure_ascii=False)

            for attempt in range(max_retries):
                try:
                    response_text = self.llm.process_batch(sys_prompt, f"Input Data:\n{user_content_str}")
                    
                    try:
                        clean_text = response_text.replace("```json", "").replace("```", "").strip()
                        data = json.loads(clean_text)
                        
                        if isinstance(data, dict):
                            # ÂòóË©¶Êâæ list È°ûÂûãÁöÑ value
                            values = [v for v in data.values() if isinstance(v, list)]
                            batch_result = values[0] if values else batch
                        elif isinstance(data, list):
                            batch_result = data
                        else:
                            batch_result = batch
                    except json.JSONDecodeError:
                        logger.warning(f"Batch {i} JSON Ëß£ÊûêÂ§±ÊïóÔºå‰ΩøÁî®ÂéüÊñá„ÄÇ")
                        batch_result = batch

                    if len(batch_result) != len(batch):
                        if len(batch_result) > len(batch):
                            batch_result = batch_result[:len(batch)]
                        else:
                            batch_result.extend(batch[len(batch_result):])

                    processed_texts.extend(batch_result)
                    
                    # Êõ¥Êñ∞‰∏ä‰∏ãÊñáÔºöÂèñÊúÄÂæå 3 Âè•ÁøªË≠ØÁµêÊûú
                    previous_context = batch_result[-3:]
                    break 

                except Exception as e:
                    logger.warning(f"API Error (Attempt {attempt+1}): {e}")
                    if attempt < max_retries - 1:
                        time.sleep(2 ** attempt)
                    else:
                        logger.error(f"Batch {i} Â§±ÊïóÔºå‰ΩøÁî®ÂéüÊñá„ÄÇ")
                        processed_texts.extend(batch)
                        previous_context = batch[-3:]
        
        return processed_texts

    # --- Â≠óÂπïÂêà‰Ωµ (Âæ©ÂàªÊ®£Âºè) ---
    def merge_subtitles(self, zh_path: str, en_path: str, output_path: str):
        logger.info("ÈñãÂßãÂêà‰ΩµÈõôË™ûÂ≠óÂπï (Ê®£ÂºèÂæ©ÂàªÊ®°Âºè)...")
        try:
            subs_zh = pysubs2.load(zh_path)
            subs_en = pysubs2.load(en_path)
        except Exception as e:
            logger.error(f"ËÆÄÂèñÂ≠óÂπïÊñá‰ª∂Â§±Êïó: {e}")
            return

        merged_subs = pysubs2.SSAFile()
        merged_subs.info.update(config.ASS_PARAMS)

        style_main_cfg = config.STYLE_CONFIG["main"]
        style_sec_cfg = config.STYLE_CONFIG["second"]

        merged_subs.styles[style_main_cfg["Name"]] = self._create_pysubs2_style(style_main_cfg)
        merged_subs.styles[style_sec_cfg["Name"]] = self._create_pysubs2_style(style_sec_cfg)

        logger.info(f"Ê≠£Âú®Âêà‰Ωµ {len(subs_zh)} Ë°åÂ≠óÂπï...")
        
        for z_event in subs_zh.events:
            z_start = z_event.start
            z_end = z_event.end
            z_text = z_event.text.strip()
            
            best_match_en = ""
            max_overlap = 0
            
            for e_event in subs_en.events:
                overlap_start = max(z_start, e_event.start)
                overlap_end = min(z_end, e_event.end)
                overlap = overlap_end - overlap_start
                
                if overlap > max_overlap:
                    max_overlap = overlap
                    best_match_en = e_event.text.strip()
                if e_event.start > z_end:
                    break
            
            if best_match_en:
                final_text = f"{z_text}\\N{{\\r{style_sec_cfg['Name']}}}{best_match_en}"
            else:
                final_text = z_text

            new_event = pysubs2.SSAEvent(
                start=z_start,
                end=z_end,
                text=final_text,
                style=style_main_cfg["Name"]
            )
            merged_subs.events.append(new_event)

        merged_subs.save(output_path)
        logger.info(f"‚úÖ ÈõôË™ûÂ≠óÂπïÂêà‰ΩµÂÆåÊàêÔºåÂ∑≤‰øùÂ≠òÁÇ∫: {output_path}")

    def _create_pysubs2_style(self, cfg: dict) -> pysubs2.SSAStyle:
        style = pysubs2.SSAStyle()
        style.fontname = cfg.get("Fontname", "Arial")
        style.fontsize = cfg.get("Fontsize", 20)
        style.primarycolor = pysubs2.Color(*self._parse_ass_color(cfg.get("PrimaryColour")))
        style.secondarycolor = pysubs2.Color(*self._parse_ass_color(cfg.get("SecondaryColour")))
        style.outlinecolor = pysubs2.Color(*self._parse_ass_color(cfg.get("OutlineColour")))
        style.backcolor = pysubs2.Color(*self._parse_ass_color(cfg.get("BackColour")))
        style.bold = cfg.get("Bold", 0)
        style.italic = cfg.get("Italic", 0)
        style.borderstyle = cfg.get("BorderStyle", 1)
        style.outline = cfg.get("Outline", 2)
        style.shadow = cfg.get("Shadow", 0)
        style.alignment = cfg.get("Alignment", 2)
        style.marginl = cfg.get("MarginL", 10)
        style.marginr = cfg.get("MarginR", 10)
        style.marginv = cfg.get("MarginV", 10)
        style.encoding = cfg.get("Encoding", 1)
        return style

    def _parse_ass_color(self, ass_hex: str):
        hex_str = ass_hex.replace("&H", "")
        if len(hex_str) != 8: return 255, 255, 255, 0
        return int(hex_str[6:8], 16), int(hex_str[4:6], 16), int(hex_str[2:4], 16), int(hex_str[0:2], 16)